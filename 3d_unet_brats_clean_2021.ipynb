{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from glob import glob as glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import monai\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import Compose\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "from monai.metrics import CumulativeIterationMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics.meandice import DiceMetric\n",
    "\n",
    "from utilities import save_checkpoint\n",
    "from utilities import permute_and_add_axis_to_mask, spatialpad\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "my_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = \"../data/train_minidata2021\"\n",
    "output_directory = \"../data/train_minidata2021/processed_train\"\n",
    "main_directory_val = \"../data/validation_minidata2021\"\n",
    "output_directory_val = \"../data/validation_minidata2021/processed_val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifty(directory, example_id, suffix):\n",
    "    file_path = os.path.join(directory, example_id + \"_\" + suffix + \".nii.gz\")\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"No such file or no access: '{file_path}'\")\n",
    "    return nib.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_channels(directory, example_id):\n",
    "    flair = load_nifty(directory, example_id, \"flair\")\n",
    "    t1 = load_nifty(directory, example_id, \"t1\")\n",
    "    t1ce = load_nifty(directory, example_id, \"t1ce\")\n",
    "    t2 = load_nifty(directory, example_id, \"t2\")\n",
    "    return flair, t1, t1ce, t2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_nifty(d, output_dir):\n",
    "    example_id = d.split(\"/\")[-1]\n",
    "    flair, t1, t1ce, t2 = load_channels(d, example_id)\n",
    "    affine, header = flair.affine, flair.header\n",
    "\n",
    "    # Load data for each modality\n",
    "    flair_data = get_data(flair)\n",
    "    t1_data = get_data(t1)\n",
    "    t1ce_data = get_data(t1ce)\n",
    "    t2_data = get_data(t2)\n",
    "\n",
    "    # Combine into a single volume\n",
    "    combined_volume = np.stack([flair_data, t1_data, t1ce_data, t2_data], axis=3)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Save the combined volume in the output directory\n",
    "    img_path = os.path.join(output_dir, example_id + \".nii.gz\")\n",
    "    vol = nib.Nifti1Image(combined_volume, affine, header=header)\n",
    "    nib.save(vol, img_path)\n",
    "\n",
    "    # Process segmentation if available\n",
    "    mask_path = \"\"\n",
    "    seg_path = os.path.join(d, example_id + \"_seg.nii.gz\")\n",
    "    if os.path.exists(seg_path):\n",
    "        seg = nib.load(seg_path)\n",
    "        affine, header = seg.affine, seg.header\n",
    "        seg_data = get_data(seg, \"uint8\")\n",
    "        seg_data[seg_data == 4] = 3  # Adjust labels if necessary\n",
    "        seg = nib.Nifti1Image(seg_data, affine, header=header)\n",
    "        mask_path = os.path.join(output_dir, example_id + \"_seg.nii.gz\")\n",
    "        nib.save(seg, mask_path)\n",
    "\n",
    "    return img_path, mask_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(nifty, dtype=\"int16\"):\n",
    "    if dtype == \"int16\":\n",
    "        data = np.abs(nifty.get_fdata().astype(np.int16))\n",
    "        data[data == -32768] = 0  # Handle edge cases\n",
    "        return data\n",
    "    return nifty.get_fdata().astype(np.uint8)\n",
    "# Initialize lists to store paths\n",
    "train_volume_path = []\n",
    "train_segmentation_path = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/train_minidata2021/BraTS2021_00366...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00366: ['BraTS2021_00366_t1ce.nii.gz', 'BraTS2021_00366_seg.nii.gz', 'BraTS2021_00366_t1.nii.gz', 'BraTS2021_00366_t2.nii.gz', 'BraTS2021_00366_flair.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00045...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00045: ['BraTS2021_00045_t1.nii.gz', 'BraTS2021_00045_t1ce.nii.gz', 'BraTS2021_00045_seg.nii.gz', 'BraTS2021_00045_t2.nii.gz', 'BraTS2021_00045_flair.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00359...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00359: ['BraTS2021_00359_seg.nii.gz', 'BraTS2021_00359_t1ce.nii.gz', 'BraTS2021_00359_t1.nii.gz', 'BraTS2021_00359_t2.nii.gz', 'BraTS2021_00359_flair.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00356...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00356: ['BraTS2021_00356_seg.nii.gz', 'BraTS2021_00356_t2.nii.gz', 'BraTS2021_00356_t1.nii.gz', 'BraTS2021_00356_t1ce.nii.gz', 'BraTS2021_00356_flair.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00062...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00062: ['BraTS2021_00062_flair.nii.gz', 'BraTS2021_00062_t2.nii.gz', 'BraTS2021_00062_t1ce.nii.gz', 'BraTS2021_00062_seg.nii.gz', 'BraTS2021_00062_t1.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00364...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00364: ['BraTS2021_00364_t1ce.nii.gz', 'BraTS2021_00364_seg.nii.gz', 'BraTS2021_00364_t1.nii.gz', 'BraTS2021_00364_flair.nii.gz', 'BraTS2021_00364_t2.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00352...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00352: ['BraTS2021_00352_t2.nii.gz', 'BraTS2021_00352_t1ce.nii.gz', 'BraTS2021_00352_flair.nii.gz', 'BraTS2021_00352_t1.nii.gz', 'BraTS2021_00352_seg.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00353...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00353: ['BraTS2021_00353_seg.nii.gz', 'BraTS2021_00353_t1.nii.gz', 'BraTS2021_00353_t1ce.nii.gz', 'BraTS2021_00353_t2.nii.gz', 'BraTS2021_00353_flair.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00046...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00046: ['BraTS2021_00046_seg.nii.gz', 'BraTS2021_00046_t2.nii.gz', 'BraTS2021_00046_t1ce.nii.gz', 'BraTS2021_00046_t1.nii.gz', 'BraTS2021_00046_flair.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00043...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00043: ['BraTS2021_00043_t1ce.nii.gz', 'BraTS2021_00043_flair.nii.gz', 'BraTS2021_00043_t1.nii.gz', 'BraTS2021_00043_seg.nii.gz', 'BraTS2021_00043_t2.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00360...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00360: ['BraTS2021_00360_seg.nii.gz', 'BraTS2021_00360_t1ce.nii.gz', 'BraTS2021_00360_t1.nii.gz', 'BraTS2021_00360_t2.nii.gz', 'BraTS2021_00360_flair.nii.gz']\n",
      "Processing ../data/train_minidata2021/processed_train...\n",
      "Files in ../data/train_minidata2021/processed_train: ['BraTS2021_00360_seg.nii.gz', 'BraTS2021_00378_seg.nii.gz', 'BraTS2021_00356_seg.nii.gz', 'BraTS2021_00046_seg.nii.gz', 'BraTS2021_00356.nii.gz', 'BraTS2021_00359_seg.nii.gz', 'BraTS2021_00062.nii.gz', 'BraTS2021_00046.nii.gz', 'BraTS2021_00352.nii.gz', 'BraTS2021_00044.nii.gz', 'BraTS2021_00379.nii.gz', 'BraTS2021_00043.nii.gz', 'BraTS2021_00364_seg.nii.gz', 'BraTS2021_00340.nii.gz', 'BraTS2021_00044_seg.nii.gz', 'BraTS2021_00045_seg.nii.gz', 'BraTS2021_00043_seg.nii.gz', 'BraTS2021_00045.nii.gz', 'BraTS2021_00366_seg.nii.gz', 'BraTS2021_00353.nii.gz', 'BraTS2021_00378.nii.gz', 'BraTS2021_00353_seg.nii.gz', 'BraTS2021_00364.nii.gz', 'BraTS2021_00366.nii.gz', 'BraTS2021_00380.nii.gz', 'BraTS2021_00340_seg.nii.gz', 'BraTS2021_00380_seg.nii.gz', 'BraTS2021_00352_seg.nii.gz', 'BraTS2021_00379_seg.nii.gz', 'BraTS2021_00062_seg.nii.gz', 'BraTS2021_00359.nii.gz', 'BraTS2021_00090.nii.gz', 'BraTS2021_00090_seg.nii.gz', 'BraTS2021_00360.nii.gz']\n",
      "No such file or no access: '../data/train_minidata2021/processed_train/processed_train_flair.nii.gz'\n",
      "Processing ../data/train_minidata2021/BraTS2021_00044...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00044: ['BraTS2021_00044_flair.nii.gz', 'BraTS2021_00044_seg.nii.gz', 'BraTS2021_00044_t1ce.nii.gz', 'BraTS2021_00044_t1.nii.gz', 'BraTS2021_00044_t2.nii.gz']\n",
      "Train Volume Paths: ['../data/train_minidata2021/processed_train/BraTS2021_00366.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00045.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00359.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00356.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00062.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00364.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00352.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00353.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00046.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00043.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00360.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00044.nii.gz']\n",
      "Train Segmentation Paths: ['../data/train_minidata2021/processed_train/BraTS2021_00366_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00045_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00359_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00356_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00062_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00364_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00352_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00353_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00046_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00043_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00360_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00044_seg.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "for subject_dir in os.listdir(main_directory):\n",
    "    subject_path = os.path.join(main_directory, subject_dir)\n",
    "    if os.path.isdir(subject_path):  # Check if it's a directory\n",
    "        print(f\"Processing {subject_path}...\")\n",
    "        try:\n",
    "            # List the contents of the directory\n",
    "            files = os.listdir(subject_path)\n",
    "            print(f\"Files in {subject_path}: {files}\")\n",
    "            \n",
    "            img_path, mask_path = prepare_nifty(subject_path, output_directory)\n",
    "            train_volume_path.append(img_path)\n",
    "            train_segmentation_path.append(mask_path if mask_path else None)\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)  # Print the error if any file is missing\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {subject_path}: {e}\")\n",
    "\n",
    "# Convert lists to arrays\n",
    "train_volume_path = np.array(train_volume_path)\n",
    "train_segmentation_path = np.array(train_segmentation_path)\n",
    "\n",
    "# Print the arrays\n",
    "print(\"Train Volume Paths:\", train_volume_path)\n",
    "print(\"Train Segmentation Paths:\", train_segmentation_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_volumes_path = train_volume_path\n",
    "train_segmentations_path = train_segmentation_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['../data/train_minidata2021/processed_train/BraTS2021_00366.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00045.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00359.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00356.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00062.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00364.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00352.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00353.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00046.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00043.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00360.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00044.nii.gz'],\n",
       "      dtype='<U65')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_volumes_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['../data/train_minidata2021/processed_train/BraTS2021_00366_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00045_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00359_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00356_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00062_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00364_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00352_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00353_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00046_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00043_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00360_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00044_seg.nii.gz'],\n",
       "      dtype='<U69')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_segmentations_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_volumes_path = []\n",
    "val_segmentations_path = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/validation_minidata2021/BraTS2021_00090...\n",
      "Files in ../data/validation_minidata2021/BraTS2021_00090: ['BraTS2021_00090_t1ce.nii.gz', 'BraTS2021_00090_flair.nii.gz', 'BraTS2021_00090_t2.nii.gz', 'BraTS2021_00090_t1.nii.gz', 'BraTS2021_00090_seg.nii.gz']\n",
      "Processing ../data/validation_minidata2021/BraTS2021_00379...\n",
      "Files in ../data/validation_minidata2021/BraTS2021_00379: ['BraTS2021_00379_t2.nii.gz', 'BraTS2021_00379_t1.nii.gz', 'BraTS2021_00379_flair.nii.gz', 'BraTS2021_00379_t1ce.nii.gz', 'BraTS2021_00379_seg.nii.gz']\n",
      "Processing ../data/validation_minidata2021/BraTS2021_00380...\n",
      "Files in ../data/validation_minidata2021/BraTS2021_00380: ['BraTS2021_00380_t1.nii.gz', 'BraTS2021_00380_flair.nii.gz', 'BraTS2021_00380_t1ce.nii.gz', 'BraTS2021_00380_t2.nii.gz', 'BraTS2021_00380_seg.nii.gz']\n",
      "Processing ../data/validation_minidata2021/BraTS2021_00378...\n",
      "Files in ../data/validation_minidata2021/BraTS2021_00378: ['BraTS2021_00378_seg.nii.gz', 'BraTS2021_00378_flair.nii.gz', 'BraTS2021_00378_t1ce.nii.gz', 'BraTS2021_00378_t1.nii.gz', 'BraTS2021_00378_t2.nii.gz']\n",
      "Processing ../data/validation_minidata2021/BraTS2021_00340...\n",
      "Files in ../data/validation_minidata2021/BraTS2021_00340: ['BraTS2021_00340_t1ce.nii.gz', 'BraTS2021_00340_t2.nii.gz', 'BraTS2021_00340_flair.nii.gz', 'BraTS2021_00340_seg.nii.gz', 'BraTS2021_00340_t1.nii.gz']\n",
      "val Volume Paths: ['../data/train_minidata2021/processed_train/BraTS2021_00090.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00379.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00380.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00378.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00340.nii.gz']\n",
      "val Segmentation Paths: ['../data/train_minidata2021/processed_train/BraTS2021_00090_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00379_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00380_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00378_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00340_seg.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "for subject_dir in os.listdir(main_directory_val):\n",
    "    subject_path = os.path.join(main_directory_val, subject_dir)\n",
    "    if os.path.isdir(subject_path):  # Check if it's a directory\n",
    "        print(f\"Processing {subject_path}...\")\n",
    "        try:\n",
    "            # List the contents of the directory\n",
    "            files = os.listdir(subject_path)\n",
    "            print(f\"Files in {subject_path}: {files}\")\n",
    "            \n",
    "            img_path, mask_path = prepare_nifty(subject_path, output_directory)\n",
    "            val_volumes_path.append(img_path)\n",
    "            val_segmentations_path.append(mask_path if mask_path else None)\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)  # Print the error if any file is missing\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {subject_path}: {e}\")\n",
    "\n",
    "# Convert lists to arrays\n",
    "train_volume_path = np.array(val_volumes_path)\n",
    "train_segmentation_path = np.array(val_segmentations_path)\n",
    "\n",
    "# Print the arrays\n",
    "print(\"val Volume Paths:\", train_volume_path)\n",
    "print(\"val Segmentation Paths:\", train_segmentation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/train_minidata2021/processed_train/BraTS2021_00090_seg.nii.gz',\n",
       " '../data/train_minidata2021/processed_train/BraTS2021_00379_seg.nii.gz',\n",
       " '../data/train_minidata2021/processed_train/BraTS2021_00380_seg.nii.gz',\n",
       " '../data/train_minidata2021/processed_train/BraTS2021_00378_seg.nii.gz',\n",
       " '../data/train_minidata2021/processed_train/BraTS2021_00340_seg.nii.gz']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_segmentations_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/train_minidata2021/processed_train/BraTS2021_00090.nii.gz',\n",
       " '../data/train_minidata2021/processed_train/BraTS2021_00379.nii.gz',\n",
       " '../data/train_minidata2021/processed_train/BraTS2021_00380.nii.gz',\n",
       " '../data/train_minidata2021/processed_train/BraTS2021_00378.nii.gz',\n",
       " '../data/train_minidata2021/processed_train/BraTS2021_00340.nii.gz']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_volumes_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('../data/train_minidata2021/processed_train/BraTS2021_00366.nii.gz',\n",
       "  '../data/train_minidata2021/processed_train/BraTS2021_00366_seg.nii.gz'),\n",
       " ('../data/train_minidata2021/processed_train/BraTS2021_00045.nii.gz',\n",
       "  '../data/train_minidata2021/processed_train/BraTS2021_00045_seg.nii.gz')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(train_volumes_path,train_segmentations_path))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BratsDataset(Dataset):\n",
    "    def __init__(self, images_path_list, masks_path_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_path_list (list of strings): List of paths to input images.\n",
    "            masks_path_list (list of strings): List of paths to masks.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.images_path_list = images_path_list\n",
    "        self.masks_path_list = masks_path_list\n",
    "        self.transform = transform\n",
    "        self.length = len(images_path_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image_path = self.images_path_list[idx]\n",
    "        image = nib.load(image_path).get_fdata()\n",
    "        image = np.float32(image) # shape of image [240, 240, 155, 4]\n",
    "\n",
    "        # Load mask\n",
    "        mask_path = self.masks_path_list[idx]\n",
    "        mask = nib.load(mask_path).get_fdata()\n",
    "        mask = np.float32(mask) # shape of mask [240, 240, 155]\n",
    "\n",
    "        if self.transform:\n",
    "            transformed_sample = self.transform({'image': image, 'mask': mask})\n",
    "        \n",
    "        return transformed_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = Compose([ # input image of shape [240, 240, 155, 4]\n",
    "    permute_and_add_axis_to_mask(), # image: [4, 155, 240, 240], mask[1, 155, 240, 240] # new channel in the first dimension is added in mask inorder to make compatible with Resize() as Resize takes only 4D tensor\n",
    "    spatialpad(image_target_size=[4, 256, 256, 256], mask_target_size=[1, 256, 256, 256]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BratsDataset(\n",
    "    train_volumes_path,\n",
    "    train_segmentations_path,\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "val_ds = BratsDataset(\n",
    "    val_volumes_path,\n",
    "    val_segmentations_path,\n",
    "    transform=data_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(dataset=train_ds,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_ds,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (model): Sequential(\n",
      "    (0): Convolution(\n",
      "      (conv): Conv3d(4, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "      (adn): ADN(\n",
      "        (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (D): Dropout(p=0.0, inplace=False)\n",
      "        (A): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (1): SkipConnection(\n",
      "      (submodule): Sequential(\n",
      "        (0): Convolution(\n",
      "          (conv): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (adn): ADN(\n",
      "            (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (D): Dropout(p=0.0, inplace=False)\n",
      "            (A): PReLU(num_parameters=1)\n",
      "          )\n",
      "        )\n",
      "        (1): SkipConnection(\n",
      "          (submodule): Sequential(\n",
      "            (0): Convolution(\n",
      "              (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "              (adn): ADN(\n",
      "                (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                (D): Dropout(p=0.0, inplace=False)\n",
      "                (A): PReLU(num_parameters=1)\n",
      "              )\n",
      "            )\n",
      "            (1): SkipConnection(\n",
      "              (submodule): Sequential(\n",
      "                (0): Convolution(\n",
      "                  (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "                  (adn): ADN(\n",
      "                    (N): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                    (D): Dropout(p=0.0, inplace=False)\n",
      "                    (A): PReLU(num_parameters=1)\n",
      "                  )\n",
      "                )\n",
      "                (1): SkipConnection(\n",
      "                  (submodule): Convolution(\n",
      "                    (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "                    (adn): ADN(\n",
      "                      (N): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                      (D): Dropout(p=0.0, inplace=False)\n",
      "                      (A): PReLU(num_parameters=1)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (2): Convolution(\n",
      "                  (conv): ConvTranspose3d(384, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "                  (adn): ADN(\n",
      "                    (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                    (D): Dropout(p=0.0, inplace=False)\n",
      "                    (A): PReLU(num_parameters=1)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (2): Convolution(\n",
      "              (conv): ConvTranspose3d(128, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "              (adn): ADN(\n",
      "                (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                (D): Dropout(p=0.0, inplace=False)\n",
      "                (A): PReLU(num_parameters=1)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): Convolution(\n",
      "          (conv): ConvTranspose3d(64, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "          (adn): ADN(\n",
      "            (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (D): Dropout(p=0.0, inplace=False)\n",
      "            (A): PReLU(num_parameters=1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Convolution(\n",
      "      (conv): ConvTranspose3d(32, 4, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a U-Net model\n",
    "model = UNet(\n",
    "    spatial_dims=3,        # 3 for using 3D ConvNet and 3D Maxpooling\n",
    "    in_channels=4,         # since 4 modalities\n",
    "    out_channels=4,        # 4 sub-regions to segment\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    ").to(my_device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model,\n",
    "               dataloader,\n",
    "               loss_fn,\n",
    "               dice_score,\n",
    "               optimizer,\n",
    "               batch_train_loss_csv,\n",
    "               epoch_num):\n",
    "               \n",
    "    # Putting the model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize train_loss list\n",
    "    train_loss = []\n",
    "\n",
    "    # Loop through batches of data\n",
    "    for batch_num, batch_data in enumerate(dataloader):\n",
    "        X = batch_data['image'] # torch.Size([batch, 4, 128, 240, 240])\n",
    "        Y = batch_data['mask'] # torch.Size([batch, 1, 128, 240, 240]) (batch, 1, 128, 240, 240) (multi-class i.e. a pixel_value ~ {0, 1, 2, 3})\n",
    "\n",
    "        # Send data to target device\n",
    "        X, Y = X.to(my_device), Y.to(my_device)\n",
    "\n",
    "        optimizer.zero_grad() # Clear previous gradients\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X) # y_pred shape torch.Size([batch, 4, 128, 240, 240]) # produces raw logits. 4 is due to 4 sub regions, not 4 modalities.\n",
    "        \n",
    "        # Compute and accumulate loss\n",
    "        loss = loss_fn(y_pred, Y) # loss one-hot encodes the y so y will be [batch, 4, 128, 240, 240] and y_pred is [batch, 4, 128, 240, 240], loss is scalar (may be averages across modalities and batch as well)\n",
    "\n",
    "        # Backpropagation and Optimization\n",
    "        loss.backward() # Compute gradients\n",
    "        optimizer.step() # Update weights\n",
    "        tr_loss = loss.item()\n",
    "\n",
    "        # Accumulate train_loss for log\n",
    "        train_loss.append(tr_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Calculate and accumulate metric across the batch\n",
    "            predicted_class_labels = torch.argmax(y_pred, dim=1, keepdim=True) # After argmax with keepdim=True: [batch, 1, D, H, W] {0, 1, 2, 3}, since it takes argmax along the channels(or, the #classes)\n",
    "\n",
    "        # Write batch wise train loss to csv file\n",
    "        with open(batch_train_loss_csv, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch_num+1, batch_num+1, tr_loss])\n",
    "\n",
    "        print(f'Iteration: {batch_num + 1} ---|---  Loss {tr_loss:.3f}')\n",
    "\n",
    "    # Average the dice loss (i.e. average of batches ~ 1 epoch)\n",
    "    train_loss_average = np.mean(train_loss) # for checkpoint\n",
    "\n",
    "    return  train_loss_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(model,\n",
    "              dataloader,\n",
    "              dice_score:CumulativeIterationMetric):\n",
    "\n",
    "    # Putting model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize validation dice score\n",
    "    val_dice_score = 0\n",
    "\n",
    "    dice_score.reset()\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode(): # Disable gradient computation \n",
    "\n",
    "        # Loop through batches of data in dataloader\n",
    "        for batch_num, batch_data in enumerate(dataloader):\n",
    "            X = batch_data['image'] # [batch, 4, D, H, W]\n",
    "            Y = batch_data['mask'] # [B, 1, D, H, W]\n",
    "\n",
    "            # Send data to target device\n",
    "            X, Y = X.to(my_device), Y.to(my_device)\n",
    "\n",
    "            # Forward pass\n",
    "            test_pred_logits = model(X) # [B, 4, 128, 240, 240]\n",
    "\n",
    "            # Calculate and accumulate metric across the batch\n",
    "            predicted_class_labels = torch.argmax(test_pred_logits, dim=1, keepdim=True) # test_pred_logits of shape [batch, 4, D, H, W] {raw logits}, after argmax [batch, D, H, W] {0, 1, 2, 3}, since it takes argmax along the channels(or, the #classes)\n",
    "            batch_dice_score = dice_score(predicted_class_labels, Y)\n",
    "            \n",
    "            # print(f\"DSC (batch wise): {batch_dice_score}\")\n",
    "\n",
    "    # Aggregate dice score (epoch wise)\n",
    "    val_dice_score = dice_score.aggregate().item()\n",
    "\n",
    "    return val_dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Various parameters required for training and test step\n",
    "def train(model,\n",
    "          checkpoint_dir,\n",
    "          batch_train_loss_csv,\n",
    "          epoch_val_dsc_csv,\n",
    "          train_loader,\n",
    "          val_loader,\n",
    "          optimizer,\n",
    "          loss_fn,\n",
    "          dice_score,\n",
    "          epochs):\n",
    "    \n",
    "    # # Creating empty list to hold loss and dice_score\n",
    "    # results = {\n",
    "    #     'train_loss':[], # This store epoch wise dice loss\n",
    "    #     'val_dice_score':[] # This stores epoch wise validation dice score\n",
    "    # }\n",
    "\n",
    "    # Looping through traininig and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f'----------- Epoch: {epoch+1} ----------- \\n')\n",
    "\n",
    "        train_loss = train_step(model=model,\n",
    "                                      dataloader=train_loader,\n",
    "                                      loss_fn=loss_fn,\n",
    "                                      dice_score=dice_score,\n",
    "                                      optimizer=optimizer,\n",
    "                                      batch_train_loss_csv=batch_train_loss_csv,\n",
    "                                      epoch_num=epoch)\n",
    "        \n",
    "        val_dice_score = val_step(model=model,\n",
    "                                  dataloader=val_loader,\n",
    "                                  dice_score=dice_score)\n",
    "\n",
    "        # Write val_dice_score to csv file\n",
    "        with open(epoch_val_dsc_csv, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch+1, val_dice_score])\n",
    "\n",
    "        print(f'--|-- Epoch {epoch+1} Validation DS: {val_dice_score:.4f} --|--')\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(model, checkpoint_dir, optimizer, epoch+1, np.mean(train_loss), val_dice_score)\n",
    "\n",
    "        # # Append to the list\n",
    "        # results['train_loss'].append(np.mean(train_loss))\n",
    "        # results['val_dice_score'].append(val_dice_score)\n",
    "\n",
    "    # return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints will be saved in: logs_and_checkpoints/logs/3DUNet/2024-05-23_21-46\n",
      "Logs will be saved in: logs_and_checkpoints/checkpoints/3DUNet/2024-05-23_21-46\n"
     ]
    }
   ],
   "source": [
    "# Model name\n",
    "model_name = '3DUNet'\n",
    "\n",
    "# Generate a timestamp\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M') # -%S\n",
    "\n",
    "# path directory to save the checkpoints and logs\n",
    "log_dir = os.path.join('logs_and_checkpoints', 'logs',model_name, timestamp)\n",
    "checkpoint_dir = os.path.join('logs_and_checkpoints', 'checkpoints', model_name, timestamp)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoints will be saved in: {log_dir}\")\n",
    "print(f\"Logs will be saved in: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a312aafc4c94f4587cb87779d26ea80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Epoch: 1 ----------- \n",
      "\n",
      "Iteration: 1 ---|---  Loss 0.996\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m start_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m model_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mbatch_train_loss_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mepoch_val_dsc_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdice_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdice_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdice_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# End the timer and print out how long it took\u001b[39;00m\n\u001b[1;32m     47\u001b[0m end_time \u001b[38;5;241m=\u001b[39m timer()\n",
      "Cell \u001b[0;32mIn[134], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, checkpoint_dir, batch_train_loss_csv, epoch_val_dsc_csv, train_loader, val_loader, optimizer, loss_fn, dice_score, epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------- Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ----------- \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mdice_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdice_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mbatch_train_loss_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_train_loss_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mepoch_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     val_dice_score \u001b[38;5;241m=\u001b[39m val_step(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     34\u001b[0m                               dataloader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[1;32m     35\u001b[0m                               dice_score\u001b[38;5;241m=\u001b[39mdice_score)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Write val_dice_score to csv file\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[132], line 26\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, dice_score, optimizer, batch_train_loss_csv, epoch_num)\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Clear previous gradients\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# y_pred shape torch.Size([batch, 4, 128, 240, 240]) # produces raw logits. 4 is due to 4 sub regions, not 4 modalities.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Compute and accumulate loss\u001b[39;00m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, Y) \u001b[38;5;66;03m# loss one-hot encodes the y so y will be [batch, 4, 128, 240, 240] and y_pred is [batch, 4, 128, 240, 240], loss is scalar (may be averages across modalities and batch as well)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/monai/networks/nets/unet.py:300\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 300\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:1108\u001b[0m, in \u001b[0;36mConvTranspose3d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1103\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m   1104\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "monai.utils.set_determinism(seed=random_seed)\n",
    "\n",
    "# Set the number of epochs, loss function and optimizer\n",
    "num_epochs = 500\n",
    "dice_loss = DiceLoss(include_background=False, to_onehot_y=True, softmax=True)\n",
    "dice_score = DiceMetric(include_background=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Create files for logs\n",
    "# batch_train_loss_csv to save batch wise loss\n",
    "# epoch_val_dsc_csv to save epoch wise loss\n",
    "batch_train_loss_csv = os.path.join(log_dir, f'batch_train_loss_{log_dir[-16:]}.csv')\n",
    "epoch_val_dsc_csv = os.path.join(log_dir, f'epoch_val_dsc_{log_dir[-16:]}.csv')\n",
    "\n",
    "with open(batch_train_loss_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Epoch', 'Iteration', 'Train Loss'])\n",
    "\n",
    "with open(epoch_val_dsc_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Epoch', 'Val DSC'])\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "# Train model\n",
    "model_results = train(model,\n",
    "                      checkpoint_dir,\n",
    "                      batch_train_loss_csv,\n",
    "                      epoch_val_dsc_csv,\n",
    "                      train_loader=train_loader,\n",
    "                      val_loader=val_loader,\n",
    "                      optimizer=optimizer,\n",
    "                      loss_fn=dice_loss,\n",
    "                      dice_score=dice_score,\n",
    "                      epochs=num_epochs)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
