{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from glob import glob as glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import monai\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import Compose\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "from monai.metrics import CumulativeIterationMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics.meandice import DiceMetric\n",
    "\n",
    "from utilities import save_checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "my_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = \"../data/train_minidata2021\"\n",
    "output_directory = \"../data/train_minidata2021/processed_train\"\n",
    "main_directory_val = \"../data/validation_minidata2021\"\n",
    "output_directory_val = \"../data/validation_minidata2021/processed_val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifty(directory, example_id, suffix):\n",
    "    file_path = os.path.join(directory, example_id + \"_\" + suffix + \".nii.gz\")\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"No such file or no access: '{file_path}'\")\n",
    "    return nib.load(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_channels(directory, example_id):\n",
    "    flair = load_nifty(directory, example_id, \"flair\")\n",
    "    t1 = load_nifty(directory, example_id, \"t1\")\n",
    "    t1ce = load_nifty(directory, example_id, \"t1ce\")\n",
    "    t2 = load_nifty(directory, example_id, \"t2\")\n",
    "    return flair, t1, t1ce, t2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_nifty(d, output_dir):\n",
    "    example_id = d.split(\"/\")[-1]\n",
    "    flair, t1, t1ce, t2 = load_channels(d, example_id)\n",
    "    affine, header = flair.affine, flair.header\n",
    "\n",
    "    # Load data for each modality\n",
    "    flair_data = get_data(flair)\n",
    "    t1_data = get_data(t1)\n",
    "    t1ce_data = get_data(t1ce)\n",
    "    t2_data = get_data(t2)\n",
    "\n",
    "    # Combine into a single volume\n",
    "    combined_volume = np.stack([flair_data, t1_data, t1ce_data, t2_data], axis=3)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Save the combined volume in the output directory\n",
    "    img_path = os.path.join(output_dir, example_id + \".nii.gz\")\n",
    "    vol = nib.Nifti1Image(combined_volume, affine, header=header)\n",
    "    nib.save(vol, img_path)\n",
    "\n",
    "    # Process segmentation if available\n",
    "    mask_path = \"\"\n",
    "    seg_path = os.path.join(d, example_id + \"_seg.nii.gz\")\n",
    "    if os.path.exists(seg_path):\n",
    "        seg = nib.load(seg_path)\n",
    "        affine, header = seg.affine, seg.header\n",
    "        seg_data = get_data(seg, \"uint8\")\n",
    "        seg_data[seg_data == 4] = 3  # Adjust labels if necessary\n",
    "        seg = nib.Nifti1Image(seg_data, affine, header=header)\n",
    "        mask_path = os.path.join(output_dir, example_id + \"_seg.nii.gz\")\n",
    "        nib.save(seg, mask_path)\n",
    "\n",
    "    return img_path, mask_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(nifty, dtype=\"int16\"):\n",
    "    if dtype == \"int16\":\n",
    "        data = np.abs(nifty.get_fdata().astype(np.int16))\n",
    "        data[data == -32768] = 0  # Handle edge cases\n",
    "        return data\n",
    "    return nifty.get_fdata().astype(np.uint8)\n",
    "# Initialize lists to store paths\n",
    "train_volume_path = []\n",
    "train_segmentation_path = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/train_minidata2021/BraTS2021_00366...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00366: ['BraTS2021_00366_t1ce.nii.gz', 'BraTS2021_00366_seg.nii.gz', 'BraTS2021_00366_t1.nii.gz', 'BraTS2021_00366_t2.nii.gz', 'BraTS2021_00366_flair.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00045...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00045: ['BraTS2021_00045_t1.nii.gz', 'BraTS2021_00045_t1ce.nii.gz', 'BraTS2021_00045_seg.nii.gz', 'BraTS2021_00045_t2.nii.gz', 'BraTS2021_00045_flair.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00359...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00359: ['BraTS2021_00359_seg.nii.gz', 'BraTS2021_00359_t1ce.nii.gz', 'BraTS2021_00359_t1.nii.gz', 'BraTS2021_00359_t2.nii.gz', 'BraTS2021_00359_flair.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00356...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00356: ['BraTS2021_00356_seg.nii.gz', 'BraTS2021_00356_t2.nii.gz', 'BraTS2021_00356_t1.nii.gz', 'BraTS2021_00356_t1ce.nii.gz', 'BraTS2021_00356_flair.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00062...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00062: ['BraTS2021_00062_flair.nii.gz', 'BraTS2021_00062_t2.nii.gz', 'BraTS2021_00062_t1ce.nii.gz', 'BraTS2021_00062_seg.nii.gz', 'BraTS2021_00062_t1.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00364...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00364: ['BraTS2021_00364_t1ce.nii.gz', 'BraTS2021_00364_seg.nii.gz', 'BraTS2021_00364_t1.nii.gz', 'BraTS2021_00364_flair.nii.gz', 'BraTS2021_00364_t2.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00352...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00352: ['BraTS2021_00352_t2.nii.gz', 'BraTS2021_00352_t1ce.nii.gz', 'BraTS2021_00352_flair.nii.gz', 'BraTS2021_00352_t1.nii.gz', 'BraTS2021_00352_seg.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00353...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00353: ['BraTS2021_00353_seg.nii.gz', 'BraTS2021_00353_t1.nii.gz', 'BraTS2021_00353_t1ce.nii.gz', 'BraTS2021_00353_t2.nii.gz', 'BraTS2021_00353_flair.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00046...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00046: ['BraTS2021_00046_seg.nii.gz', 'BraTS2021_00046_t2.nii.gz', 'BraTS2021_00046_t1ce.nii.gz', 'BraTS2021_00046_t1.nii.gz', 'BraTS2021_00046_flair.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00043...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00043: ['BraTS2021_00043_t1ce.nii.gz', 'BraTS2021_00043_flair.nii.gz', 'BraTS2021_00043_t1.nii.gz', 'BraTS2021_00043_seg.nii.gz', 'BraTS2021_00043_t2.nii.gz']\n",
      "Processing ../data/train_minidata2021/BraTS2021_00360...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00360: ['BraTS2021_00360_seg.nii.gz', 'BraTS2021_00360_t1ce.nii.gz', 'BraTS2021_00360_t1.nii.gz', 'BraTS2021_00360_t2.nii.gz', 'BraTS2021_00360_flair.nii.gz']\n",
      "Processing ../data/train_minidata2021/processed_train...\n",
      "Files in ../data/train_minidata2021/processed_train: ['BraTS2021_00360_seg.nii.gz', 'BraTS2021_00378_seg.nii.gz', 'BraTS2021_00356_seg.nii.gz', 'BraTS2021_00046_seg.nii.gz', 'BraTS2021_00356.nii.gz', 'BraTS2021_00359_seg.nii.gz', 'BraTS2021_00062.nii.gz', 'BraTS2021_00046.nii.gz', 'BraTS2021_00352.nii.gz', 'BraTS2021_00044.nii.gz', 'BraTS2021_00379.nii.gz', 'BraTS2021_00043.nii.gz', 'BraTS2021_00364_seg.nii.gz', 'BraTS2021_00340.nii.gz', 'BraTS2021_00044_seg.nii.gz', 'BraTS2021_00045_seg.nii.gz', 'BraTS2021_00043_seg.nii.gz', 'BraTS2021_00045.nii.gz', 'BraTS2021_00366_seg.nii.gz', 'BraTS2021_00353.nii.gz', 'BraTS2021_00378.nii.gz', 'BraTS2021_00353_seg.nii.gz', 'BraTS2021_00364.nii.gz', 'BraTS2021_00366.nii.gz', 'BraTS2021_00380.nii.gz', 'BraTS2021_00340_seg.nii.gz', 'BraTS2021_00380_seg.nii.gz', 'BraTS2021_00352_seg.nii.gz', 'BraTS2021_00379_seg.nii.gz', 'BraTS2021_00062_seg.nii.gz', 'BraTS2021_00359.nii.gz', 'BraTS2021_00090.nii.gz', 'BraTS2021_00090_seg.nii.gz', 'BraTS2021_00360.nii.gz']\n",
      "No such file or no access: '../data/train_minidata2021/processed_train/processed_train_flair.nii.gz'\n",
      "Processing ../data/train_minidata2021/BraTS2021_00044...\n",
      "Files in ../data/train_minidata2021/BraTS2021_00044: ['BraTS2021_00044_flair.nii.gz', 'BraTS2021_00044_seg.nii.gz', 'BraTS2021_00044_t1ce.nii.gz', 'BraTS2021_00044_t1.nii.gz', 'BraTS2021_00044_t2.nii.gz']\n",
      "Train Volume Paths: ['../data/train_minidata2021/processed_train/BraTS2021_00366.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00045.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00359.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00356.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00062.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00364.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00352.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00353.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00046.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00043.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00360.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00044.nii.gz']\n",
      "Train Segmentation Paths: ['../data/train_minidata2021/processed_train/BraTS2021_00366_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00045_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00359_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00356_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00062_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00364_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00352_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00353_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00046_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00043_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00360_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00044_seg.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "for subject_dir in os.listdir(main_directory):\n",
    "    subject_path = os.path.join(main_directory, subject_dir)\n",
    "    if os.path.isdir(subject_path):  # Check if it's a directory\n",
    "        print(f\"Processing {subject_path}...\")\n",
    "        try:\n",
    "            # List the contents of the directory\n",
    "            files = os.listdir(subject_path)\n",
    "            print(f\"Files in {subject_path}: {files}\")\n",
    "            \n",
    "            img_path, mask_path = prepare_nifty(subject_path, output_directory)\n",
    "            train_volume_path.append(img_path)\n",
    "            train_segmentation_path.append(mask_path if mask_path else None)\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)  # Print the error if any file is missing\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {subject_path}: {e}\")\n",
    "\n",
    "# Convert lists to arrays\n",
    "train_volume_path = np.array(train_volume_path)\n",
    "train_segmentation_path = np.array(train_segmentation_path)\n",
    "\n",
    "# Print the arrays\n",
    "print(\"Train Volume Paths:\", train_volume_path)\n",
    "print(\"Train Segmentation Paths:\", train_segmentation_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_volumes_path = train_volume_path\n",
    "train_segmentations_path = train_segmentation_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['../data/train_minidata2021/processed_train/BraTS2021_00366.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00045.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00359.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00356.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00062.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00364.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00352.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00353.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00046.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00043.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00360.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00044.nii.gz'],\n",
       "      dtype='<U65')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_volumes_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['../data/train_minidata2021/processed_train/BraTS2021_00366_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00045_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00359_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00356_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00062_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00364_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00352_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00353_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00046_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00043_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00360_seg.nii.gz',\n",
       "       '../data/train_minidata2021/processed_train/BraTS2021_00044_seg.nii.gz'],\n",
       "      dtype='<U69')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_segmentations_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_volumes_path = []\n",
    "val_segmentations_path = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/validation_minidata2021/BraTS2021_00090...\n",
      "Files in ../data/validation_minidata2021/BraTS2021_00090: ['BraTS2021_00090_t1ce.nii.gz', 'BraTS2021_00090_flair.nii.gz', 'BraTS2021_00090_t2.nii.gz', 'BraTS2021_00090_t1.nii.gz', 'BraTS2021_00090_seg.nii.gz']\n",
      "Processing ../data/validation_minidata2021/BraTS2021_00379...\n",
      "Files in ../data/validation_minidata2021/BraTS2021_00379: ['BraTS2021_00379_t2.nii.gz', 'BraTS2021_00379_t1.nii.gz', 'BraTS2021_00379_flair.nii.gz', 'BraTS2021_00379_t1ce.nii.gz', 'BraTS2021_00379_seg.nii.gz']\n",
      "Processing ../data/validation_minidata2021/BraTS2021_00380...\n",
      "Files in ../data/validation_minidata2021/BraTS2021_00380: ['BraTS2021_00380_t1.nii.gz', 'BraTS2021_00380_flair.nii.gz', 'BraTS2021_00380_t1ce.nii.gz', 'BraTS2021_00380_t2.nii.gz', 'BraTS2021_00380_seg.nii.gz']\n",
      "Processing ../data/validation_minidata2021/BraTS2021_00378...\n",
      "Files in ../data/validation_minidata2021/BraTS2021_00378: ['BraTS2021_00378_seg.nii.gz', 'BraTS2021_00378_flair.nii.gz', 'BraTS2021_00378_t1ce.nii.gz', 'BraTS2021_00378_t1.nii.gz', 'BraTS2021_00378_t2.nii.gz']\n",
      "Processing ../data/validation_minidata2021/BraTS2021_00340...\n",
      "Files in ../data/validation_minidata2021/BraTS2021_00340: ['BraTS2021_00340_t1ce.nii.gz', 'BraTS2021_00340_t2.nii.gz', 'BraTS2021_00340_flair.nii.gz', 'BraTS2021_00340_seg.nii.gz', 'BraTS2021_00340_t1.nii.gz']\n",
      "val Volume Paths: ['../data/train_minidata2021/processed_train/BraTS2021_00090.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00379.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00380.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00378.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00340.nii.gz']\n",
      "val Segmentation Paths: ['../data/train_minidata2021/processed_train/BraTS2021_00090_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00379_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00380_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00378_seg.nii.gz'\n",
      " '../data/train_minidata2021/processed_train/BraTS2021_00340_seg.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "for subject_dir in os.listdir(main_directory_val):\n",
    "    subject_path = os.path.join(main_directory_val, subject_dir)\n",
    "    if os.path.isdir(subject_path):  # Check if it's a directory\n",
    "        print(f\"Processing {subject_path}...\")\n",
    "        try:\n",
    "            # List the contents of the directory\n",
    "            files = os.listdir(subject_path)\n",
    "            print(f\"Files in {subject_path}: {files}\")\n",
    "            \n",
    "            img_path, mask_path = prepare_nifty(subject_path, output_directory)\n",
    "            val_volumes_path.append(img_path)\n",
    "            val_segmentations_path.append(mask_path if mask_path else None)\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)  # Print the error if any file is missing\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {subject_path}: {e}\")\n",
    "\n",
    "# Convert lists to arrays\n",
    "train_volume_path = np.array(val_volumes_path)\n",
    "train_segmentation_path = np.array(val_segmentations_path)\n",
    "\n",
    "# Print the arrays\n",
    "print(\"val Volume Paths:\", train_volume_path)\n",
    "print(\"val Segmentation Paths:\", train_segmentation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/train_minidata2021/processed_train/BraTS2021_00090_seg.nii.gz',\n",
       " '../data/train_minidata2021/processed_train/BraTS2021_00379_seg.nii.gz',\n",
       " '../data/train_minidata2021/processed_train/BraTS2021_00380_seg.nii.gz',\n",
       " '../data/train_minidata2021/processed_train/BraTS2021_00378_seg.nii.gz',\n",
       " '../data/train_minidata2021/processed_train/BraTS2021_00340_seg.nii.gz']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_segmentations_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/train_minidata2021/processed_train/BraTS2021_00090.nii.gz',\n",
       " '../data/train_minidata2021/processed_train/BraTS2021_00379.nii.gz',\n",
       " '../data/train_minidata2021/processed_train/BraTS2021_00380.nii.gz',\n",
       " '../data/train_minidata2021/processed_train/BraTS2021_00378.nii.gz',\n",
       " '../data/train_minidata2021/processed_train/BraTS2021_00340.nii.gz']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_volumes_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('../data/train_minidata2021/processed_train/BraTS2021_00366.nii.gz',\n",
       "  '../data/train_minidata2021/processed_train/BraTS2021_00366_seg.nii.gz'),\n",
       " ('../data/train_minidata2021/processed_train/BraTS2021_00045.nii.gz',\n",
       "  '../data/train_minidata2021/processed_train/BraTS2021_00045_seg.nii.gz')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(train_volumes_path,train_segmentations_path))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class permute_and_add_axis_to_mask(object):\n",
    "    def __call__(self, sample):\n",
    "        # Previous: (240, 240, 155, 4) , need to change to (4, 155, 240, 240) i.e. (channel, depth, height, width)\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        image = image.transpose((3, 2, 0, 1))\n",
    "        mask = mask.transpose((2, 0, 1))\n",
    "\n",
    "        mask= mask[np.newaxis, ...]\n",
    "        return {'image':image,\n",
    "                'mask':mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BratsDataset(Dataset):\n",
    "    def __init__(self, images_path_list, masks_path_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_path_list (list of strings): List of paths to input images.\n",
    "            masks_path_list (list of strings): List of paths to masks.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.images_path_list = images_path_list\n",
    "        self.masks_path_list = masks_path_list\n",
    "        self.transform = transform\n",
    "        self.length = len(images_path_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image_path = self.images_path_list[idx]\n",
    "        image = nib.load(image_path).get_fdata()\n",
    "        image = np.float32(image) # shape of image [240, 240, 155, 4]\n",
    "\n",
    "        # Load mask\n",
    "        mask_path = self.masks_path_list[idx]\n",
    "        mask = nib.load(mask_path).get_fdata()\n",
    "        mask = np.float32(mask) # shape of mask [240, 240, 155]\n",
    "\n",
    "        if self.transform:\n",
    "            transformed_sample = self.transform({'image': image, 'mask': mask})\n",
    "        \n",
    "        return transformed_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spatialpad(object): # First dimension should be left untouched of [C, D, H, W]\n",
    "    def __init__(self, image_target_size=[4, 256, 256, 256], mask_target_size=[1, 256, 256, 256]):\n",
    "        self.image_target_size = image_target_size\n",
    "        self.mask_target_size = mask_target_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample['image'], sample['mask'] # image: [4, 155, 240, 240], mask[1, 155, 240, 240]\n",
    "\n",
    "        padded_image = self.pad_input(image, self.image_target_size)\n",
    "\n",
    "        padded_mask = self.pad_input(mask, self.mask_target_size)\n",
    "\n",
    "        return {'image': padded_image,\n",
    "                'mask': padded_mask}\n",
    "    \n",
    "\n",
    "    def pad_input(self, input_array, target_size):\n",
    "        # Ensure the input array is a numpy array\n",
    "        if not isinstance(input_array, np.ndarray):\n",
    "            input_array = np.array(input_array)\n",
    "\n",
    "        # Calculate padding sizes for each dimension\n",
    "        pad_width = []\n",
    "        for i in range(len(input_array.shape)):\n",
    "            total_padding = target_size[i] - input_array.shape[i]\n",
    "            if total_padding < 0:\n",
    "                raise ValueError(f\"Target shape must be larger than the input shape. Dimension {i} is too small.\")\n",
    "            pad_before = total_padding // 2\n",
    "            pad_after = total_padding - pad_before\n",
    "            pad_width.append((pad_before, pad_after))\n",
    "\n",
    "        # Pad the image\n",
    "        padded_image = np.pad(input_array, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "        return padded_image   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = Compose([ # input image of shape [240, 240, 155, 4]\n",
    "    permute_and_add_axis_to_mask(), # image: [4, 155, 240, 240], mask[1, 155, 240, 240] # new channel in the first dimension is added in mask inorder to make compatible with Resize() as Resize takes only 4D tensor\n",
    "    spatialpad(image_target_size=[4, 256, 256, 256], mask_target_size=[1, 256, 256, 256]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BratsDataset(\n",
    "    train_volumes_path,\n",
    "    train_segmentations_path,\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "val_ds = BratsDataset(\n",
    "    val_volumes_path,\n",
    "    val_segmentations_path,\n",
    "    transform=data_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 256, 256, 256), (1, 256, 256, 256))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train = train_ds[0]\n",
    "sample_train['image'].shape, sample_train['mask'].shape # previously numpy array of (240, 240, 155, 4), Now changed to: (4, 155, 240, 240) with first transform, then changed to (4, 256, 256, 256) by second transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "train_loader = DataLoader(dataset=train_ds,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_ds,\n",
    "                        batch_size=1,\n",
    "                        shuffle=False,\n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (model): Sequential(\n",
      "    (0): Convolution(\n",
      "      (conv): Conv3d(4, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "      (adn): ADN(\n",
      "        (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (D): Dropout(p=0.0, inplace=False)\n",
      "        (A): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (1): SkipConnection(\n",
      "      (submodule): Sequential(\n",
      "        (0): Convolution(\n",
      "          (conv): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (adn): ADN(\n",
      "            (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (D): Dropout(p=0.0, inplace=False)\n",
      "            (A): PReLU(num_parameters=1)\n",
      "          )\n",
      "        )\n",
      "        (1): SkipConnection(\n",
      "          (submodule): Sequential(\n",
      "            (0): Convolution(\n",
      "              (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "              (adn): ADN(\n",
      "                (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                (D): Dropout(p=0.0, inplace=False)\n",
      "                (A): PReLU(num_parameters=1)\n",
      "              )\n",
      "            )\n",
      "            (1): SkipConnection(\n",
      "              (submodule): Sequential(\n",
      "                (0): Convolution(\n",
      "                  (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "                  (adn): ADN(\n",
      "                    (N): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                    (D): Dropout(p=0.0, inplace=False)\n",
      "                    (A): PReLU(num_parameters=1)\n",
      "                  )\n",
      "                )\n",
      "                (1): SkipConnection(\n",
      "                  (submodule): Convolution(\n",
      "                    (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "                    (adn): ADN(\n",
      "                      (N): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                      (D): Dropout(p=0.0, inplace=False)\n",
      "                      (A): PReLU(num_parameters=1)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (2): Convolution(\n",
      "                  (conv): ConvTranspose3d(384, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "                  (adn): ADN(\n",
      "                    (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                    (D): Dropout(p=0.0, inplace=False)\n",
      "                    (A): PReLU(num_parameters=1)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (2): Convolution(\n",
      "              (conv): ConvTranspose3d(128, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "              (adn): ADN(\n",
      "                (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                (D): Dropout(p=0.0, inplace=False)\n",
      "                (A): PReLU(num_parameters=1)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): Convolution(\n",
      "          (conv): ConvTranspose3d(64, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "          (adn): ADN(\n",
      "            (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (D): Dropout(p=0.0, inplace=False)\n",
      "            (A): PReLU(num_parameters=1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Convolution(\n",
      "      (conv): ConvTranspose3d(32, 4, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a U-Net model\n",
    "model = UNet(\n",
    "    spatial_dims=3,        # 3 for using 3D ConvNet and 3D Maxpooling\n",
    "    in_channels=4,         # since 4 modalities\n",
    "    out_channels=4,        # 4 sub-regions to segment\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    ").to(my_device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model,\n",
    "               dataloader,\n",
    "               loss_fn,\n",
    "               dice_score,\n",
    "               optimizer):\n",
    "    # Putting the model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize train_loss list\n",
    "    train_loss = [] # epoch wise\n",
    "\n",
    "    # Loop through batches of data\n",
    "    for batch_num, batch_data in enumerate(dataloader):\n",
    "        X = batch_data['image'] # torch.Size([batch, 4, 128, 240, 240])\n",
    "        Y = batch_data['mask'] # torch.Size([batch, 1, 128, 240, 240]) (batch, 1, 128, 240, 240) (multi-class i.e. a pixel_value ~ {0, 1, 2, 3})\n",
    "\n",
    "        # Send data to target device\n",
    "        X, Y = X.to(my_device), Y.to(my_device)\n",
    "\n",
    "        optimizer.zero_grad() # Clear previous gradients\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X) # y_pred shape torch.Size([batch, 4, 128, 240, 240]) # produces raw logits. 4 is due to 4 sub regions, not 4 modalities.\n",
    "        \n",
    "        # Compute and accumulate loss\n",
    "        loss = loss_fn(y_pred, Y) # loss one-hot encodes the y so y will be [batch, 4, 128, 240, 240] and y_pred is [batch, 4, 128, 240, 240], loss is scalar (may be averages across modalities and batch as well)\n",
    "\n",
    "        # Backpropagation and Optimization\n",
    "        loss.backward() # Compute gradients\n",
    "        optimizer.step() # Update weights\n",
    "        tr_loss = loss.item()\n",
    "\n",
    "        # Accumulate train_loss for log\n",
    "        train_loss.append(tr_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Calculate and accumulate metric across the batch\n",
    "            predicted_class_labels = torch.argmax(y_pred, dim=1, keepdim=True) # After argmax with keepdim=True: [batch, 1, D, H, W] {0, 1, 2, 3}, since it takes argmax along the channels(or, the #classes)\n",
    "\n",
    "        print(f'Iteration: {batch_num + 1} ---|---  Loss {tr_loss:.3f}')\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(model,\n",
    "              dataloader,\n",
    "              dice_score:CumulativeIterationMetric):\n",
    "    \n",
    "    # Putting model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize validation dice score\n",
    "    val_dice_score = 0\n",
    "\n",
    "    dice_score.reset()\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode(): # Disable gradient computation \n",
    "\n",
    "        # Loop through batches of data in dataloader\n",
    "        for batch_num, batch_data in enumerate(dataloader):\n",
    "            X = batch_data['image'] # [batch, 4, D, H, W]\n",
    "            Y = batch_data['mask'] # [B, 1, D, H, W]\n",
    "\n",
    "            # Send data to target device\n",
    "            X, Y = X.to(my_device), Y.to(my_device)\n",
    "\n",
    "            # Forward pass\n",
    "            test_pred_logits = model(X) # [B, 4, 128, 240, 240]\n",
    "\n",
    "            # Calculate and accumulate metric across the batch\n",
    "            predicted_class_labels = torch.argmax(test_pred_logits, dim=1, keepdim=True) # test_pred_logits of shape [batch, 4, D, H, W] {raw logits}, after argmax [batch, D, H, W] {0, 1, 2, 3}, since it takes argmax along the channels(or, the #classes)\n",
    "            batch_dice_score = dice_score(predicted_class_labels, Y)\n",
    "            \n",
    "            # print(f\"DSC (batch wise): {batch_dice_score}\")\n",
    "\n",
    "    # Aggregate dice score (epoch wise)\n",
    "    val_dice_score = dice_score.aggregate().item()\n",
    "\n",
    "    return val_dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Various parameters required for training and test step\n",
    "def train(model,\n",
    "          checkpoint_dir,\n",
    "          train_loader,\n",
    "          val_loader,\n",
    "          optimizer,\n",
    "          loss_fn,\n",
    "          dice_score,\n",
    "          epochs):\n",
    "    \n",
    "    # Creating empty list to hold loss and accuracy\n",
    "    results = {\n",
    "        'batch_train_loss':[],\n",
    "        'epoch_val_dice_score':[]\n",
    "    }\n",
    "\n",
    "    # Looping through traininig and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f'----------- Epoch: {epoch+1} ----------- \\n')\n",
    "        batch_train_loss_list = train_step(model=model,\n",
    "                                      dataloader=train_loader,\n",
    "                                      loss_fn=loss_fn,\n",
    "                                      dice_score=dice_score,\n",
    "                                      optimizer=optimizer)\n",
    "        \n",
    "        epoch_val_dice_score = val_step(model=model,\n",
    "                                  dataloader=val_loader,\n",
    "                                  dice_score=dice_score)\n",
    "\n",
    "        print(f'\\n'\n",
    "              f'--|-- Epoch {epoch+1} Validation DS: {epoch_val_dice_score:.4f} --|--')\n",
    "        \n",
    "        # Save checkpoint\n",
    "        # save_checkpoint(model, checkpoint_dir, optimizer, epoch, 0.0, 0.0, torch.mean(batch_train_loss_list), epoch_val_dice_score)\n",
    "\n",
    "        # Append to the list\n",
    "        results['batch_train_loss'].append(np.mean(batch_train_loss_list))\n",
    "        results['epoch_val_dice_score'].append(epoch_val_dice_score)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints will be saved in: checkpoints/3DUNet/2024-05-23_17-18-12\n"
     ]
    }
   ],
   "source": [
    "# Model name\n",
    "model_name = '3DUNet'\n",
    "\n",
    "# Generate a timestamp\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# Directory to save the checkpoints\n",
    "checkpoint_dir = os.path.join('checkpoints', model_name, timestamp)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoints will be saved in: {checkpoint_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4868be24cb49aeb1c6215a22b0d4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Epoch: 1 ----------- \n",
      "\n",
      "Iteration: 1 ---|---  Loss 0.996\n",
      "Iteration: 2 ---|---  Loss 0.997\n",
      "Iteration: 3 ---|---  Loss 0.998\n",
      "Iteration: 4 ---|---  Loss 0.996\n",
      "Iteration: 5 ---|---  Loss 0.997\n",
      "Iteration: 6 ---|---  Loss 0.996\n",
      "Iteration: 7 ---|---  Loss 0.998\n",
      "Iteration: 8 ---|---  Loss 0.996\n",
      "Iteration: 9 ---|---  Loss 0.999\n",
      "Iteration: 10 ---|---  Loss 0.994\n",
      "Iteration: 11 ---|---  Loss 0.991\n",
      "Iteration: 12 ---|---  Loss 0.997\n",
      "\n",
      "--|-- Epoch 1 Validation DS: 0.0000 --|--\n",
      "----------- Epoch: 2 ----------- \n",
      "\n",
      "Iteration: 1 ---|---  Loss 0.985\n",
      "Iteration: 2 ---|---  Loss 0.996\n",
      "Iteration: 3 ---|---  Loss 0.993\n",
      "Iteration: 4 ---|---  Loss 0.996\n",
      "Iteration: 5 ---|---  Loss 0.994\n",
      "Iteration: 6 ---|---  Loss 0.990\n",
      "Iteration: 7 ---|---  Loss 0.996\n",
      "Iteration: 8 ---|---  Loss 0.998\n",
      "Iteration: 9 ---|---  Loss 0.998\n",
      "Iteration: 10 ---|---  Loss 0.996\n",
      "Iteration: 11 ---|---  Loss 0.994\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "monai.utils.set_determinism(seed=random_seed)\n",
    "\n",
    "# Set the number of epochs, loss function and optimizer\n",
    "num_epochs = 2\n",
    "dice_loss = DiceLoss(include_background=False, to_onehot_y=True, softmax=True)\n",
    "dice_score = DiceMetric(include_background=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "# Train model\n",
    "model_results = train(model,\n",
    "                      checkpoint_dir,\n",
    "                      train_loader=train_loader,\n",
    "                      val_loader=val_loader,\n",
    "                      optimizer=optimizer,\n",
    "                      loss_fn=dice_loss,\n",
    "                      dice_score=dice_score,\n",
    "                      epochs=num_epochs)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
