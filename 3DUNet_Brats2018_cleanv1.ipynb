{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\envs\\mbase\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from glob import glob as glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import monai\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import Compose\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "from monai.metrics import CumulativeIterationMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics.meandice import DiceMetric\n",
    "\n",
    "from utilities import save_checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "my_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = 'D:\\\\Neuroscience and Neuroimaging\\\\CAP5516 Medical Image Computing\\\\MSD\\\\brats_subset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_volumes_path = sorted(glob(os.path.join(local_dir, 'TrainVolumes', '*.nii.gz')))\n",
    "train_segmentations_path = sorted(glob(os.path.join(local_dir, 'TrainSegmentation', '*.nii.gz')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('D:\\\\Neuroscience and Neuroimaging\\\\CAP5516 Medical Image Computing\\\\MSD\\\\brats_subset\\\\TrainVolumes\\\\BRATS_001.nii.gz',\n",
       "  'D:\\\\Neuroscience and Neuroimaging\\\\CAP5516 Medical Image Computing\\\\MSD\\\\brats_subset\\\\TrainSegmentation\\\\BRATS_001.nii.gz'),\n",
       " ('D:\\\\Neuroscience and Neuroimaging\\\\CAP5516 Medical Image Computing\\\\MSD\\\\brats_subset\\\\TrainVolumes\\\\BRATS_002.nii.gz',\n",
       "  'D:\\\\Neuroscience and Neuroimaging\\\\CAP5516 Medical Image Computing\\\\MSD\\\\brats_subset\\\\TrainSegmentation\\\\BRATS_002.nii.gz')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(train_volumes_path,train_segmentations_path))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_volumes_path = sorted(glob(os.path.join(local_dir, 'TestVolumes', '*.nii.gz')))\n",
    "val_segmentations_path = sorted(glob(os.path.join(local_dir, 'TestSegmentation', '*.nii.gz')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('D:\\\\Neuroscience and Neuroimaging\\\\CAP5516 Medical Image Computing\\\\MSD\\\\brats_subset\\\\TestVolumes\\\\BRATS_011.nii.gz',\n",
       "  'D:\\\\Neuroscience and Neuroimaging\\\\CAP5516 Medical Image Computing\\\\MSD\\\\brats_subset\\\\TestSegmentation\\\\BRATS_011.nii.gz'),\n",
       " ('D:\\\\Neuroscience and Neuroimaging\\\\CAP5516 Medical Image Computing\\\\MSD\\\\brats_subset\\\\TestVolumes\\\\BRATS_012.nii.gz',\n",
       "  'D:\\\\Neuroscience and Neuroimaging\\\\CAP5516 Medical Image Computing\\\\MSD\\\\brats_subset\\\\TestSegmentation\\\\BRATS_012.nii.gz'),\n",
       " ('D:\\\\Neuroscience and Neuroimaging\\\\CAP5516 Medical Image Computing\\\\MSD\\\\brats_subset\\\\TestVolumes\\\\BRATS_013.nii.gz',\n",
       "  'D:\\\\Neuroscience and Neuroimaging\\\\CAP5516 Medical Image Computing\\\\MSD\\\\brats_subset\\\\TestSegmentation\\\\BRATS_013.nii.gz'),\n",
       " ('D:\\\\Neuroscience and Neuroimaging\\\\CAP5516 Medical Image Computing\\\\MSD\\\\brats_subset\\\\TestVolumes\\\\BRATS_014.nii.gz',\n",
       "  'D:\\\\Neuroscience and Neuroimaging\\\\CAP5516 Medical Image Computing\\\\MSD\\\\brats_subset\\\\TestSegmentation\\\\BRATS_014.nii.gz')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(val_volumes_path,val_segmentations_path))[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class permute_and_add_axis_to_mask(object):\n",
    "    def __call__(self, sample):\n",
    "        # Previous: (240, 240, 155, 4) , need to change to (4, 155, 240, 240) i.e. (channel, depth, height, width)\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        image = image.transpose((3, 2, 0, 1))\n",
    "        mask = mask.transpose((2, 0, 1))\n",
    "\n",
    "        mask= mask[np.newaxis, ...]\n",
    "        return {'image':image,\n",
    "                'mask':mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BratsDataset(Dataset):\n",
    "    def __init__(self, images_path_list, masks_path_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_path_list (list of strings): List of paths to input images.\n",
    "            masks_path_list (list of strings): List of paths to masks.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.images_path_list = images_path_list\n",
    "        self.masks_path_list = masks_path_list\n",
    "        self.transform = transform\n",
    "        self.length = len(images_path_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image_path = self.images_path_list[idx]\n",
    "        image = nib.load(image_path).get_fdata()\n",
    "        image = np.float32(image) # shape of image [240, 240, 155, 4]\n",
    "\n",
    "        # Load mask\n",
    "        mask_path = self.masks_path_list[idx]\n",
    "        mask = nib.load(mask_path).get_fdata()\n",
    "        mask = np.float32(mask) # shape of mask [240, 240, 155]\n",
    "\n",
    "        if self.transform:\n",
    "            transformed_sample = self.transform({'image': image, 'mask': mask})\n",
    "        \n",
    "        return transformed_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spatialpad(object): # First dimension should be left untouched of [C, D, H, W]\n",
    "    def __init__(self, image_target_size=[4, 256, 256, 256], mask_target_size=[1, 256, 256, 256]):\n",
    "        self.image_target_size = image_target_size\n",
    "        self.mask_target_size = mask_target_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample['image'], sample['mask'] # image: [4, 155, 240, 240], mask[1, 155, 240, 240]\n",
    "\n",
    "        padded_image = self.pad_input(image, self.image_target_size)\n",
    "\n",
    "        padded_mask = self.pad_input(mask, self.mask_target_size)\n",
    "\n",
    "        return {'image': padded_image,\n",
    "                'mask': padded_mask}\n",
    "    \n",
    "\n",
    "    def pad_input(self, input_array, target_size):\n",
    "        # Ensure the input array is a numpy array\n",
    "        if not isinstance(input_array, np.ndarray):\n",
    "            input_array = np.array(input_array)\n",
    "\n",
    "        # Calculate padding sizes for each dimension\n",
    "        pad_width = []\n",
    "        for i in range(len(input_array.shape)):\n",
    "            total_padding = target_size[i] - input_array.shape[i]\n",
    "            if total_padding < 0:\n",
    "                raise ValueError(f\"Target shape must be larger than the input shape. Dimension {i} is too small.\")\n",
    "            pad_before = total_padding // 2\n",
    "            pad_after = total_padding - pad_before\n",
    "            pad_width.append((pad_before, pad_after))\n",
    "\n",
    "        # Pad the image\n",
    "        padded_image = np.pad(input_array, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "        return padded_image   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = Compose([ # input image of shape [240, 240, 155, 4]\n",
    "    permute_and_add_axis_to_mask(), # image: [4, 155, 240, 240], mask[1, 155, 240, 240] # new channel in the first dimension is added in mask inorder to make compatible with Resize() as Resize takes only 4D tensor\n",
    "    spatialpad(image_target_size=[4, 256, 256, 256], mask_target_size=[1, 256, 256, 256]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BratsDataset(\n",
    "    train_volumes_path,\n",
    "    train_segmentations_path,\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "val_ds = BratsDataset(\n",
    "    val_volumes_path,\n",
    "    val_segmentations_path,\n",
    "    transform=data_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 256, 256, 256), (1, 256, 256, 256))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train = train_ds[0]\n",
    "sample_train['image'].shape, sample_train['mask'].shape # previously numpy array of (240, 240, 155, 4), Now changed to: (4, 155, 240, 240) with first transform, then changed to (4, 256, 256, 256) by second transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "train_loader = DataLoader(dataset=train_ds,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_ds,\n",
    "                        batch_size=1,\n",
    "                        shuffle=False,\n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (model): Sequential(\n",
      "    (0): Convolution(\n",
      "      (conv): Conv3d(4, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "      (adn): ADN(\n",
      "        (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (D): Dropout(p=0.0, inplace=False)\n",
      "        (A): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (1): SkipConnection(\n",
      "      (submodule): Sequential(\n",
      "        (0): Convolution(\n",
      "          (conv): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (adn): ADN(\n",
      "            (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (D): Dropout(p=0.0, inplace=False)\n",
      "            (A): PReLU(num_parameters=1)\n",
      "          )\n",
      "        )\n",
      "        (1): SkipConnection(\n",
      "          (submodule): Sequential(\n",
      "            (0): Convolution(\n",
      "              (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "              (adn): ADN(\n",
      "                (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                (D): Dropout(p=0.0, inplace=False)\n",
      "                (A): PReLU(num_parameters=1)\n",
      "              )\n",
      "            )\n",
      "            (1): SkipConnection(\n",
      "              (submodule): Sequential(\n",
      "                (0): Convolution(\n",
      "                  (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "                  (adn): ADN(\n",
      "                    (N): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                    (D): Dropout(p=0.0, inplace=False)\n",
      "                    (A): PReLU(num_parameters=1)\n",
      "                  )\n",
      "                )\n",
      "                (1): SkipConnection(\n",
      "                  (submodule): Convolution(\n",
      "                    (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "                    (adn): ADN(\n",
      "                      (N): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                      (D): Dropout(p=0.0, inplace=False)\n",
      "                      (A): PReLU(num_parameters=1)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (2): Convolution(\n",
      "                  (conv): ConvTranspose3d(384, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "                  (adn): ADN(\n",
      "                    (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                    (D): Dropout(p=0.0, inplace=False)\n",
      "                    (A): PReLU(num_parameters=1)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (2): Convolution(\n",
      "              (conv): ConvTranspose3d(128, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "              (adn): ADN(\n",
      "                (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                (D): Dropout(p=0.0, inplace=False)\n",
      "                (A): PReLU(num_parameters=1)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): Convolution(\n",
      "          (conv): ConvTranspose3d(64, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "          (adn): ADN(\n",
      "            (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (D): Dropout(p=0.0, inplace=False)\n",
      "            (A): PReLU(num_parameters=1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Convolution(\n",
      "      (conv): ConvTranspose3d(32, 4, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a U-Net model\n",
    "model = UNet(\n",
    "    spatial_dims=3,        # 3 for using 3D ConvNet and 3D Maxpooling\n",
    "    in_channels=4,         # since 4 modalities\n",
    "    out_channels=4,        # 4 sub-regions to segment\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    ").to(my_device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model,\n",
    "               dataloader,\n",
    "               loss_fn,\n",
    "               dice_score,\n",
    "               optimizer):\n",
    "    # Putting the model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize train_loss list\n",
    "    train_loss = [] # epoch wise\n",
    "\n",
    "    # Loop through batches of data\n",
    "    for batch_num, batch_data in enumerate(dataloader):\n",
    "        X = batch_data['image'] # torch.Size([batch, 4, 128, 240, 240])\n",
    "        Y = batch_data['mask'] # torch.Size([batch, 1, 128, 240, 240]) (batch, 1, 128, 240, 240) (multi-class i.e. a pixel_value ~ {0, 1, 2, 3})\n",
    "\n",
    "        # Send data to target device\n",
    "        X, Y = X.to(my_device), Y.to(my_device)\n",
    "\n",
    "        optimizer.zero_grad() # Clear previous gradients\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X) # y_pred shape torch.Size([batch, 4, 128, 240, 240]) # produces raw logits. 4 is due to 4 sub regions, not 4 modalities.\n",
    "        \n",
    "        # Compute and accumulate loss\n",
    "        loss = loss_fn(y_pred, Y) # loss one-hot encodes the y so y will be [batch, 4, 128, 240, 240] and y_pred is [batch, 4, 128, 240, 240], loss is scalar (may be averages across modalities and batch as well)\n",
    "\n",
    "        # Backpropagation and Optimization\n",
    "        loss.backward() # Compute gradients\n",
    "        optimizer.step() # Update weights\n",
    "        tr_loss = loss.item()\n",
    "\n",
    "        # Accumulate train_loss for log\n",
    "        train_loss.append(tr_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Calculate and accumulate metric across the batch\n",
    "            predicted_class_labels = torch.argmax(y_pred, dim=1, keepdim=True) # After argmax with keepdim=True: [batch, 1, D, H, W] {0, 1, 2, 3}, since it takes argmax along the channels(or, the #classes)\n",
    "\n",
    "        print(f'Iteration: {batch_num + 1} ---|---  Loss {tr_loss:.3f}')\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(model,\n",
    "              dataloader,\n",
    "              dice_score:CumulativeIterationMetric):\n",
    "    \n",
    "    # Putting model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize validation dice score\n",
    "    val_dice_score = 0\n",
    "\n",
    "    dice_score.reset()\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode(): # Disable gradient computation \n",
    "\n",
    "        # Loop through batches of data in dataloader\n",
    "        for batch_num, batch_data in enumerate(dataloader):\n",
    "            X = batch_data['image'] # [batch, 4, D, H, W]\n",
    "            Y = batch_data['mask'] # [B, 1, D, H, W]\n",
    "\n",
    "            # Send data to target device\n",
    "            X, Y = X.to(my_device), Y.to(my_device)\n",
    "\n",
    "            # Forward pass\n",
    "            test_pred_logits = model(X) # [B, 4, 128, 240, 240]\n",
    "\n",
    "            # Calculate and accumulate metric across the batch\n",
    "            predicted_class_labels = torch.argmax(test_pred_logits, dim=1, keepdim=True) # test_pred_logits of shape [batch, 4, D, H, W] {raw logits}, after argmax [batch, D, H, W] {0, 1, 2, 3}, since it takes argmax along the channels(or, the #classes)\n",
    "            batch_dice_score = dice_score(predicted_class_labels, Y)\n",
    "            \n",
    "            # print(f\"DSC (batch wise): {batch_dice_score}\")\n",
    "\n",
    "    # Aggregate dice score (epoch wise)\n",
    "    val_dice_score = dice_score.aggregate().item()\n",
    "\n",
    "    return val_dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Various parameters required for training and test step\n",
    "def train(model,\n",
    "          checkpoint_dir,\n",
    "          train_loader,\n",
    "          val_loader,\n",
    "          optimizer,\n",
    "          loss_fn,\n",
    "          dice_score,\n",
    "          epochs):\n",
    "    \n",
    "    # Creating empty list to hold loss and accuracy\n",
    "    results = {\n",
    "        'batch_train_loss':[],\n",
    "        'epoch_val_dice_score':[]\n",
    "    }\n",
    "\n",
    "    # Looping through traininig and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f'----------- Epoch: {epoch+1} ----------- \\n')\n",
    "        batch_train_loss_list = train_step(model=model,\n",
    "                                      dataloader=train_loader,\n",
    "                                      loss_fn=loss_fn,\n",
    "                                      dice_score=dice_score,\n",
    "                                      optimizer=optimizer)\n",
    "        \n",
    "        epoch_val_dice_score = val_step(model=model,\n",
    "                                  dataloader=val_loader,\n",
    "                                  dice_score=dice_score)\n",
    "\n",
    "        print(f'\\n'\n",
    "              f'--|-- Epoch {epoch+1} Validation DS: {epoch_val_dice_score:.4f} --|--')\n",
    "        \n",
    "        # Save checkpoint\n",
    "        # save_checkpoint(model, checkpoint_dir, optimizer, epoch, 0.0, 0.0, torch.mean(batch_train_loss_list), epoch_val_dice_score)\n",
    "\n",
    "        # Append to the list\n",
    "        results['batch_train_loss'].append(np.mean(batch_train_loss_list))\n",
    "        results['epoch_val_dice_score'].append(epoch_val_dice_score)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints will be saved in: checkpoints\\3DUNet\\2024-05-23_17-09-53\n"
     ]
    }
   ],
   "source": [
    "# Model name\n",
    "model_name = '3DUNet'\n",
    "\n",
    "# Generate a timestamp\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# Directory to save the checkpoints\n",
    "checkpoint_dir = os.path.join('checkpoints', model_name, timestamp)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoints will be saved in: {checkpoint_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Epoch: 1 ----------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\envs\\mbase\\lib\\site-packages\\torch\\nn\\modules\\conv.py:1104: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv_transpose3d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 ---|---  Loss 0.993\n",
      "Iteration: 2 ---|---  Loss 0.995\n",
      "Iteration: 3 ---|---  Loss 0.999\n",
      "Iteration: 4 ---|---  Loss 0.996\n",
      "Iteration: 5 ---|---  Loss 0.995\n",
      "Iteration: 6 ---|---  Loss 0.987\n",
      "Iteration: 7 ---|---  Loss 0.995\n",
      "Iteration: 8 ---|---  Loss 0.993\n",
      "Iteration: 9 ---|---  Loss 0.982\n",
      "Iteration: 10 ---|---  Loss 0.983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:43<00:43, 43.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--|-- Epoch 1 Validation DS: 0.0664 --|--\n",
      "----------- Epoch: 2 ----------- \n",
      "\n",
      "Iteration: 1 ---|---  Loss 0.995\n",
      "Iteration: 2 ---|---  Loss 0.998\n",
      "Iteration: 3 ---|---  Loss 0.993\n",
      "Iteration: 4 ---|---  Loss 0.978\n",
      "Iteration: 5 ---|---  Loss 0.991\n",
      "Iteration: 6 ---|---  Loss 0.978\n",
      "Iteration: 7 ---|---  Loss 0.994\n",
      "Iteration: 8 ---|---  Loss 0.978\n",
      "Iteration: 9 ---|---  Loss 0.991\n",
      "Iteration: 10 ---|---  Loss 0.982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:25<00:00, 42.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--|-- Epoch 2 Validation DS: 0.0695 --|--\n",
      "Total training time: 85.464 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "monai.utils.set_determinism(seed=random_seed)\n",
    "\n",
    "# Set the number of epochs, loss function and optimizer\n",
    "num_epochs = 2\n",
    "dice_loss = DiceLoss(include_background=False, to_onehot_y=True, softmax=True)\n",
    "dice_score = DiceMetric(include_background=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "# Train model\n",
    "model_results = train(model,\n",
    "                      checkpoint_dir,\n",
    "                      train_loader=train_loader,\n",
    "                      val_loader=val_loader,\n",
    "                      optimizer=optimizer,\n",
    "                      loss_fn=dice_loss,\n",
    "                      dice_score=dice_score,\n",
    "                      epochs=num_epochs)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NBASE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
