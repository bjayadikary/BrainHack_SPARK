{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\envs\\mbase\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from glob import glob as glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import monai\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import Compose\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "from monai.metrics import CumulativeIterationMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics.meandice import DiceMetric\n",
    "\n",
    "from utilities import save_checkpoint\n",
    "from utilities import permute_and_add_axis_to_mask, spatialpad\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "my_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_dir = \"D:\\\\Neuroscience and Neuroimaging\\\\CAP5516 Medical Image Computing\\\\MSD\\\\brats_subset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volumes_path(base_data_dir):\n",
    "    train_volumes_path = sorted(glob(os.path.join(base_data_dir, 'TrainVolumes', '*.nii.gz')))\n",
    "    train_segmentations_path = sorted(glob(os.path.join(base_data_dir, 'TrainSegmentation', '*.nii.gz')))\n",
    "\n",
    "    val_volumes_path = sorted(glob(os.path.join(base_data_dir, 'TestVolumes', '*.nii.gz')))\n",
    "    val_segmentations_path = sorted(glob(os.path.join(base_data_dir, 'TestSegmentation', '*.nii.gz')))\n",
    "\n",
    "    return train_volumes_path, train_segmentations_path, val_volumes_path, val_segmentations_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BratsDataset(Dataset):\n",
    "    def __init__(self, images_path_list, masks_path_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_path_list (list of strings): List of paths to input images.\n",
    "            masks_path_list (list of strings): List of paths to masks.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.images_path_list = images_path_list\n",
    "        self.masks_path_list = masks_path_list\n",
    "        self.transform = transform\n",
    "        self.length = len(images_path_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image_path = self.images_path_list[idx]\n",
    "        image = nib.load(image_path).get_fdata()\n",
    "        image = np.float32(image) # shape of image [240, 240, 155, 4]\n",
    "\n",
    "        # Load mask\n",
    "        mask_path = self.masks_path_list[idx]\n",
    "        mask = nib.load(mask_path).get_fdata()\n",
    "        mask = np.float32(mask) # shape of mask [240, 240, 155]\n",
    "\n",
    "        if self.transform:\n",
    "            transformed_sample = self.transform({'image': image, 'mask': mask})\n",
    "        \n",
    "        return transformed_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_volumes_path, train_segmentations_path, val_volumes_path, val_segmentations_path = get_volumes_path(base_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = Compose([ # input image of shape [240, 240, 155, 4]\n",
    "    permute_and_add_axis_to_mask(), # image: [4, 155, 240, 240], mask[1, 155, 240, 240] # new channel in the first dimension is added in mask inorder to make compatible with Resize() as Resize takes only 4D tensor\n",
    "    spatialpad(image_target_size=[4, 256, 256, 256], mask_target_size=[1, 256, 256, 256]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BratsDataset(\n",
    "    train_volumes_path,\n",
    "    train_segmentations_path,\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "val_ds = BratsDataset(\n",
    "    val_volumes_path,\n",
    "    val_segmentations_path,\n",
    "    transform=data_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(dataset=train_ds,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_ds,\n",
    "                        batch_size=1,\n",
    "                        shuffle=False,\n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (model): Sequential(\n",
      "    (0): Convolution(\n",
      "      (conv): Conv3d(4, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "      (adn): ADN(\n",
      "        (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (D): Dropout(p=0.0, inplace=False)\n",
      "        (A): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (1): SkipConnection(\n",
      "      (submodule): Sequential(\n",
      "        (0): Convolution(\n",
      "          (conv): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (adn): ADN(\n",
      "            (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (D): Dropout(p=0.0, inplace=False)\n",
      "            (A): PReLU(num_parameters=1)\n",
      "          )\n",
      "        )\n",
      "        (1): SkipConnection(\n",
      "          (submodule): Sequential(\n",
      "            (0): Convolution(\n",
      "              (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "              (adn): ADN(\n",
      "                (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                (D): Dropout(p=0.0, inplace=False)\n",
      "                (A): PReLU(num_parameters=1)\n",
      "              )\n",
      "            )\n",
      "            (1): SkipConnection(\n",
      "              (submodule): Sequential(\n",
      "                (0): Convolution(\n",
      "                  (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "                  (adn): ADN(\n",
      "                    (N): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                    (D): Dropout(p=0.0, inplace=False)\n",
      "                    (A): PReLU(num_parameters=1)\n",
      "                  )\n",
      "                )\n",
      "                (1): SkipConnection(\n",
      "                  (submodule): Convolution(\n",
      "                    (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "                    (adn): ADN(\n",
      "                      (N): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                      (D): Dropout(p=0.0, inplace=False)\n",
      "                      (A): PReLU(num_parameters=1)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (2): Convolution(\n",
      "                  (conv): ConvTranspose3d(384, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "                  (adn): ADN(\n",
      "                    (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                    (D): Dropout(p=0.0, inplace=False)\n",
      "                    (A): PReLU(num_parameters=1)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (2): Convolution(\n",
      "              (conv): ConvTranspose3d(128, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "              (adn): ADN(\n",
      "                (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                (D): Dropout(p=0.0, inplace=False)\n",
      "                (A): PReLU(num_parameters=1)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): Convolution(\n",
      "          (conv): ConvTranspose3d(64, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "          (adn): ADN(\n",
      "            (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (D): Dropout(p=0.0, inplace=False)\n",
      "            (A): PReLU(num_parameters=1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Convolution(\n",
      "      (conv): ConvTranspose3d(32, 4, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a U-Net model\n",
    "model = UNet(\n",
    "    spatial_dims=3,        # 3 for using 3D ConvNet and 3D Maxpooling\n",
    "    in_channels=4,         # since 4 modalities\n",
    "    out_channels=4,        # 4 sub-regions to segment\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    ").to(my_device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model,\n",
    "               dataloader,\n",
    "               loss_fn,\n",
    "               dice_score,\n",
    "               optimizer,\n",
    "               batch_train_loss_csv,\n",
    "               epoch_num):\n",
    "               \n",
    "    # Putting the model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize train_loss list\n",
    "    train_loss = []\n",
    "\n",
    "    # Loop through batches of data\n",
    "    for batch_num, batch_data in enumerate(dataloader):\n",
    "        X = batch_data['image'] # torch.Size([batch, 4, 128, 240, 240])\n",
    "        Y = batch_data['mask'] # torch.Size([batch, 1, 128, 240, 240]) (batch, 1, 128, 240, 240) (multi-class i.e. a pixel_value ~ {0, 1, 2, 3})\n",
    "\n",
    "        # Send data to target device\n",
    "        X, Y = X.to(my_device), Y.to(my_device)\n",
    "\n",
    "        optimizer.zero_grad() # Clear previous gradients\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X) # y_pred shape torch.Size([batch, 4, 128, 240, 240]) # produces raw logits. 4 is due to 4 sub regions, not 4 modalities.\n",
    "        \n",
    "        # Compute and accumulate loss\n",
    "        loss = loss_fn(y_pred, Y) # loss one-hot encodes the y so y will be [batch, 4, 128, 240, 240] and y_pred is [batch, 4, 128, 240, 240], loss is scalar (may be averages across modalities and batch as well)\n",
    "\n",
    "        # Backpropagation and Optimization\n",
    "        loss.backward() # Compute gradients\n",
    "        optimizer.step() # Update weights\n",
    "        tr_loss = loss.item()\n",
    "\n",
    "        # Accumulate train_loss for log\n",
    "        train_loss.append(tr_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Calculate and accumulate metric across the batch\n",
    "            predicted_class_labels = torch.argmax(y_pred, dim=1, keepdim=True) # After argmax with keepdim=True: [batch, 1, D, H, W] {0, 1, 2, 3}, since it takes argmax along the channels(or, the #classes)\n",
    "\n",
    "        # Write batch wise train loss to csv file\n",
    "        with open(batch_train_loss_csv, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch_num+1, batch_num+1, tr_loss])\n",
    "\n",
    "        print(f'Iteration: {batch_num + 1} ---|---  Loss {tr_loss:.3f}')\n",
    "\n",
    "    # Average the dice loss (i.e. average of batches ~ 1 epoch)\n",
    "    train_loss_average = np.mean(train_loss) # for checkpoint\n",
    "\n",
    "    return  train_loss_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(model,\n",
    "              dataloader,\n",
    "              dice_score:CumulativeIterationMetric):\n",
    "\n",
    "    # Putting model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize validation dice score\n",
    "    val_dice_score = 0\n",
    "\n",
    "    dice_score.reset()\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode(): # Disable gradient computation \n",
    "\n",
    "        # Loop through batches of data in dataloader\n",
    "        for batch_num, batch_data in enumerate(dataloader):\n",
    "            X = batch_data['image'] # [batch, 4, D, H, W]\n",
    "            Y = batch_data['mask'] # [B, 1, D, H, W]\n",
    "\n",
    "            # Send data to target device\n",
    "            X, Y = X.to(my_device), Y.to(my_device)\n",
    "\n",
    "            # Forward pass\n",
    "            test_pred_logits = model(X) # [B, 4, 128, 240, 240]\n",
    "\n",
    "            # Calculate and accumulate metric across the batch\n",
    "            predicted_class_labels = torch.argmax(test_pred_logits, dim=1, keepdim=True) # test_pred_logits of shape [batch, 4, D, H, W] {raw logits}, after argmax [batch, D, H, W] {0, 1, 2, 3}, since it takes argmax along the channels(or, the #classes)\n",
    "            batch_dice_score = dice_score(predicted_class_labels, Y)\n",
    "            \n",
    "            # print(f\"DSC (batch wise): {batch_dice_score}\")\n",
    "\n",
    "    # Aggregate dice score (epoch wise)\n",
    "    val_dice_score = dice_score.aggregate().item()\n",
    "\n",
    "    return val_dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Various parameters required for training and test step\n",
    "def train(model,\n",
    "          checkpoint_dir,\n",
    "          batch_train_loss_csv,\n",
    "          epoch_val_dsc_csv,\n",
    "          train_loader,\n",
    "          val_loader,\n",
    "          optimizer,\n",
    "          loss_fn,\n",
    "          dice_score,\n",
    "          epochs):\n",
    "    \n",
    "    # # Creating empty list to hold loss and dice_score\n",
    "    # results = {\n",
    "    #     'train_loss':[], # This store epoch wise dice loss\n",
    "    #     'val_dice_score':[] # This stores epoch wise validation dice score\n",
    "    # }\n",
    "\n",
    "    # Looping through traininig and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f'----------- Epoch: {epoch+1} ----------- \\n')\n",
    "\n",
    "        train_loss = train_step(model=model,\n",
    "                                      dataloader=train_loader,\n",
    "                                      loss_fn=loss_fn,\n",
    "                                      dice_score=dice_score,\n",
    "                                      optimizer=optimizer,\n",
    "                                      batch_train_loss_csv=batch_train_loss_csv,\n",
    "                                      epoch_num=epoch)\n",
    "        \n",
    "        val_dice_score = val_step(model=model,\n",
    "                                  dataloader=val_loader,\n",
    "                                  dice_score=dice_score)\n",
    "\n",
    "        # Write val_dice_score to csv file\n",
    "        with open(epoch_val_dsc_csv, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch+1, val_dice_score])\n",
    "\n",
    "        print(f'--|-- Epoch {epoch+1} Validation DS: {val_dice_score:.4f} --|--')\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(model, checkpoint_dir, optimizer, epoch+1, np.mean(train_loss), val_dice_score)\n",
    "\n",
    "        # # Append to the list\n",
    "        # results['train_loss'].append(np.mean(train_loss))\n",
    "        # results['val_dice_score'].append(val_dice_score)\n",
    "\n",
    "    # return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints will be saved in: logs_and_checkpoints\\logs\\2024-05-23_20-48\n",
      "Logs will be saved in: logs_and_checkpoints\\checkpoints\\3DUNet\\2024-05-23_20-48\n"
     ]
    }
   ],
   "source": [
    "# Model name\n",
    "model_name = '3DUNet'\n",
    "\n",
    "# Generate a timestamp\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M') # -%S\n",
    "\n",
    "# path directory to save the checkpoints and logs\n",
    "log_dir = os.path.join('logs_and_checkpoints', 'logs', timestamp)\n",
    "checkpoint_dir = os.path.join('logs_and_checkpoints', 'checkpoints', model_name, timestamp)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoints will be saved in: {log_dir}\")\n",
    "print(f\"Logs will be saved in: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Epoch: 1 ----------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\envs\\mbase\\lib\\site-packages\\torch\\nn\\modules\\conv.py:1104: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv_transpose3d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 ---|---  Loss 0.992\n",
      "Iteration: 2 ---|---  Loss 0.994\n",
      "Iteration: 3 ---|---  Loss 0.998\n",
      "Iteration: 4 ---|---  Loss 0.994\n",
      "Iteration: 5 ---|---  Loss 0.995\n",
      "Iteration: 6 ---|---  Loss 0.985\n",
      "Iteration: 7 ---|---  Loss 0.995\n",
      "Iteration: 8 ---|---  Loss 0.993\n",
      "Iteration: 9 ---|---  Loss 0.979\n",
      "Iteration: 10 ---|---  Loss 0.980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:42<06:26, 43.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--|-- Epoch 1 Validation DS: 0.0776 --|--\n",
      "Checkpoint saved at logs_and_checkpoints\\checkpoints\\3DUNet\\2024-05-23_20-48\\Epoch_1_checkpoint_2024-05-23_20-48-47.pth\n",
      "----------- Epoch: 2 ----------- \n",
      "\n",
      "Iteration: 1 ---|---  Loss 0.994\n",
      "Iteration: 2 ---|---  Loss 0.997\n",
      "Iteration: 3 ---|---  Loss 0.993\n",
      "Iteration: 4 ---|---  Loss 0.976\n",
      "Iteration: 5 ---|---  Loss 0.990\n",
      "Iteration: 6 ---|---  Loss 0.975\n",
      "Iteration: 7 ---|---  Loss 0.993\n",
      "Iteration: 8 ---|---  Loss 0.977\n",
      "Iteration: 9 ---|---  Loss 0.991\n",
      "Iteration: 10 ---|---  Loss 0.982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:24<05:38, 42.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--|-- Epoch 2 Validation DS: 0.1007 --|--\n",
      "Checkpoint saved at logs_and_checkpoints\\checkpoints\\3DUNet\\2024-05-23_20-48\\Epoch_2_checkpoint_2024-05-23_20-49-28.pth\n",
      "----------- Epoch: 3 ----------- \n",
      "\n",
      "Iteration: 1 ---|---  Loss 0.993\n",
      "Iteration: 2 ---|---  Loss 0.992\n",
      "Iteration: 3 ---|---  Loss 0.971\n",
      "Iteration: 4 ---|---  Loss 0.997\n",
      "Iteration: 5 ---|---  Loss 0.972\n",
      "Iteration: 6 ---|---  Loss 0.988\n",
      "Iteration: 7 ---|---  Loss 0.979\n",
      "Iteration: 8 ---|---  Loss 0.989\n",
      "Iteration: 9 ---|---  Loss 0.992\n",
      "Iteration: 10 ---|---  Loss 0.972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [02:06<04:53, 41.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--|-- Epoch 3 Validation DS: 0.0929 --|--\n",
      "Checkpoint saved at logs_and_checkpoints\\checkpoints\\3DUNet\\2024-05-23_20-48\\Epoch_3_checkpoint_2024-05-23_20-50-10.pth\n",
      "----------- Epoch: 4 ----------- \n",
      "\n",
      "Iteration: 1 ---|---  Loss 0.968\n",
      "Iteration: 2 ---|---  Loss 0.991\n",
      "Iteration: 3 ---|---  Loss 0.986\n",
      "Iteration: 4 ---|---  Loss 0.991\n",
      "Iteration: 5 ---|---  Loss 0.987\n",
      "Iteration: 6 ---|---  Loss 0.996\n",
      "Iteration: 7 ---|---  Loss 0.975\n",
      "Iteration: 8 ---|---  Loss 0.990\n",
      "Iteration: 9 ---|---  Loss 0.968\n",
      "Iteration: 10 ---|---  Loss 0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [02:48<04:11, 41.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--|-- Epoch 4 Validation DS: 0.1280 --|--\n",
      "Checkpoint saved at logs_and_checkpoints\\checkpoints\\3DUNet\\2024-05-23_20-48\\Epoch_4_checkpoint_2024-05-23_20-50-52.pth\n",
      "----------- Epoch: 5 ----------- \n",
      "\n",
      "Iteration: 1 ---|---  Loss 0.962\n",
      "Iteration: 2 ---|---  Loss 0.972\n",
      "Iteration: 3 ---|---  Loss 0.964\n",
      "Iteration: 4 ---|---  Loss 0.985\n",
      "Iteration: 5 ---|---  Loss 0.989\n",
      "Iteration: 6 ---|---  Loss 0.989\n",
      "Iteration: 7 ---|---  Loss 0.995\n",
      "Iteration: 8 ---|---  Loss 0.982\n",
      "Iteration: 9 ---|---  Loss 0.956\n",
      "Iteration: 10 ---|---  Loss 0.989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [03:30<03:29, 41.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--|-- Epoch 5 Validation DS: 0.1878 --|--\n",
      "Checkpoint saved at logs_and_checkpoints\\checkpoints\\3DUNet\\2024-05-23_20-48\\Epoch_5_checkpoint_2024-05-23_20-51-34.pth\n",
      "----------- Epoch: 6 ----------- \n",
      "\n",
      "Iteration: 1 ---|---  Loss 0.981\n",
      "Iteration: 2 ---|---  Loss 0.995\n",
      "Iteration: 3 ---|---  Loss 0.986\n",
      "Iteration: 4 ---|---  Loss 0.957\n",
      "Iteration: 5 ---|---  Loss 0.951\n",
      "Iteration: 6 ---|---  Loss 0.966\n",
      "Iteration: 7 ---|---  Loss 0.986\n",
      "Iteration: 8 ---|---  Loss 0.980\n",
      "Iteration: 9 ---|---  Loss 0.952\n",
      "Iteration: 10 ---|---  Loss 0.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [04:11<02:47, 41.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--|-- Epoch 6 Validation DS: 0.2357 --|--\n",
      "Checkpoint saved at logs_and_checkpoints\\checkpoints\\3DUNet\\2024-05-23_20-48\\Epoch_6_checkpoint_2024-05-23_20-52-15.pth\n",
      "----------- Epoch: 7 ----------- \n",
      "\n",
      "Iteration: 1 ---|---  Loss 0.948\n",
      "Iteration: 2 ---|---  Loss 0.961\n",
      "Iteration: 3 ---|---  Loss 0.986\n",
      "Iteration: 4 ---|---  Loss 0.983\n",
      "Iteration: 5 ---|---  Loss 0.986\n",
      "Iteration: 6 ---|---  Loss 0.942\n",
      "Iteration: 7 ---|---  Loss 0.994\n",
      "Iteration: 8 ---|---  Loss 0.975\n",
      "Iteration: 9 ---|---  Loss 0.975\n",
      "Iteration: 10 ---|---  Loss 0.942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [04:53<02:05, 41.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--|-- Epoch 7 Validation DS: 0.3107 --|--\n",
      "Checkpoint saved at logs_and_checkpoints\\checkpoints\\3DUNet\\2024-05-23_20-48\\Epoch_7_checkpoint_2024-05-23_20-52-57.pth\n",
      "----------- Epoch: 8 ----------- \n",
      "\n",
      "Iteration: 1 ---|---  Loss 0.983\n",
      "Iteration: 2 ---|---  Loss 0.973\n",
      "Iteration: 3 ---|---  Loss 0.981\n",
      "Iteration: 4 ---|---  Loss 0.974\n",
      "Iteration: 5 ---|---  Loss 0.982\n",
      "Iteration: 6 ---|---  Loss 0.950\n",
      "Iteration: 7 ---|---  Loss 0.935\n",
      "Iteration: 8 ---|---  Loss 0.992\n",
      "Iteration: 9 ---|---  Loss 0.925\n",
      "Iteration: 10 ---|---  Loss 0.932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [05:35<01:23, 41.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--|-- Epoch 8 Validation DS: 0.3232 --|--\n",
      "Checkpoint saved at logs_and_checkpoints\\checkpoints\\3DUNet\\2024-05-23_20-48\\Epoch_8_checkpoint_2024-05-23_20-53-39.pth\n",
      "----------- Epoch: 9 ----------- \n",
      "\n",
      "Iteration: 1 ---|---  Loss 0.979\n",
      "Iteration: 2 ---|---  Loss 0.980\n",
      "Iteration: 3 ---|---  Loss 0.930\n",
      "Iteration: 4 ---|---  Loss 0.990\n",
      "Iteration: 5 ---|---  Loss 0.927\n",
      "Iteration: 6 ---|---  Loss 0.965\n",
      "Iteration: 7 ---|---  Loss 0.974\n",
      "Iteration: 8 ---|---  Loss 0.913\n",
      "Iteration: 9 ---|---  Loss 0.939\n",
      "Iteration: 10 ---|---  Loss 0.963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [06:17<00:42, 42.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--|-- Epoch 9 Validation DS: 0.3286 --|--\n",
      "Checkpoint saved at logs_and_checkpoints\\checkpoints\\3DUNet\\2024-05-23_20-48\\Epoch_9_checkpoint_2024-05-23_20-54-22.pth\n",
      "----------- Epoch: 10 ----------- \n",
      "\n",
      "Iteration: 1 ---|---  Loss 0.975\n",
      "Iteration: 2 ---|---  Loss 0.919\n",
      "Iteration: 3 ---|---  Loss 0.988\n",
      "Iteration: 4 ---|---  Loss 0.913\n",
      "Iteration: 5 ---|---  Loss 0.933\n",
      "Iteration: 6 ---|---  Loss 0.957\n",
      "Iteration: 7 ---|---  Loss 0.969\n",
      "Iteration: 8 ---|---  Loss 0.959\n",
      "Iteration: 9 ---|---  Loss 0.897\n",
      "Iteration: 10 ---|---  Loss 0.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [07:00<00:00, 42.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--|-- Epoch 10 Validation DS: 0.3508 --|--\n",
      "Checkpoint saved at logs_and_checkpoints\\checkpoints\\3DUNet\\2024-05-23_20-48\\Epoch_10_checkpoint_2024-05-23_20-55-04.pth\n",
      "Total training time: 420.235 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "monai.utils.set_determinism(seed=random_seed)\n",
    "\n",
    "# Set the number of epochs, loss function and optimizer\n",
    "num_epochs = 10\n",
    "dice_loss = DiceLoss(include_background=False, to_onehot_y=True, softmax=True)\n",
    "dice_score = DiceMetric(include_background=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Create files for logs\n",
    "# batch_train_loss_csv to save batch wise loss\n",
    "# epoch_val_dsc_csv to save epoch wise loss\n",
    "batch_train_loss_csv = os.path.join(log_dir, f'batch_train_loss_{log_dir[-16:]}.csv')\n",
    "epoch_val_dsc_csv = os.path.join(log_dir, f'epoch_val_dsc_{log_dir[-16:]}.csv')\n",
    "\n",
    "with open(batch_train_loss_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Epoch', 'Iteration', 'Train Loss'])\n",
    "\n",
    "with open(epoch_val_dsc_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Epoch', 'Val DSC'])\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "# Train model\n",
    "model_results = train(model,\n",
    "                      checkpoint_dir,\n",
    "                      batch_train_loss_csv,\n",
    "                      epoch_val_dsc_csv,\n",
    "                      train_loader=train_loader,\n",
    "                      val_loader=val_loader,\n",
    "                      optimizer=optimizer,\n",
    "                      loss_fn=dice_loss,\n",
    "                      dice_score=dice_score,\n",
    "                      epochs=num_epochs)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NBASE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
